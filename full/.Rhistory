name_files_anomrr=paste0(gsub("ind","anomclim",name_index),"_T1000_avg.nc")
for (i in 1:length(res)) {
temp_index=as.numeric(unlist(res[name_index[i]]))
rr_temp_clim=subset(rr_clim,temp_index)
rr_temp_clim=calc(rr_temp_clim,mean,na.rm=T)
rr_anom_month=rr_temp_clim-rr_clim_m[[month[i]]]
writeRaster(rr_temp_clim,name_var_rr[i], "CDF", overwrite=TRUE,varname="T1000",varunit="K degree",longname="T1000",xname="lon",yname="lat")
writeRaster(rr_anom_month,name_files_anomrr[i], "CDF", overwrite=TRUE,varname="anom_T1000",varunit="degree",longname="T1000",xname="lon",yname="lat")
rr_temp_clim=subset(rr_clim,temp_index)
writeRaster(rr_temp_clim,name_var[i], "CDF", overwrite=TRUE,varname="T1000",varunit="K degree",longname="T1000",xname="lon",yname="lat")
}
###################################################################################################################################
rr_clim=brick( "r_T1000_maxday_clim.nc",lvar=1)
rr_clim_m=brick( "climean_rea2_T1000_max.nc",lvar=1)
res=readRDS("index_list_wt.rds")
name_index=names(res)
month=sort(rep(c(1:12),7))
name_var_rr=paste0(gsub("ind","clim",names(res)),"_T1000")
name_var=paste0(gsub("ind_t_wt_*_","allclim_wt_",name_index),"_T1000_max", perl = TRUE)
name_var=gsub("TRUE","",name_var)
name_files_anomrr=paste0(gsub("ind","anomclim",name_index),"_T1000_max.nc")
for (i in 1:length(res)) {
temp_index=as.numeric(unlist(res[name_index[i]]))
rr_temp_clim=subset(rr_clim,temp_index)
rr_temp_clim=calc(rr_temp_clim,mean,na.rm=T)
rr_anom_month=rr_temp_clim-rr_clim_m[[month[i]]]
writeRaster(rr_temp_clim,name_var_rr[i], "CDF", overwrite=TRUE,varname="T1000",varunit="K degree",longname="T1000",xname="lon",yname="lat")
writeRaster(rr_anom_month,name_files_anomrr[i], "CDF", overwrite=TRUE,varname="anom_T1000",varunit="degree",longname="T1000",xname="lon",yname="lat")
rr_temp_clim=subset(rr_clim,temp_index)
writeRaster(rr_temp_clim,name_var[i], "CDF", overwrite=TRUE,varname="T1000",varunit="K degree",longname="T1000",xname="lon",yname="lat")
}
###################################################################################################################################
rr_clim=brick( "r_T1000_minday_clim.nc",lvar=1)
rr_clim_m=brick( "climean_rea2_T1000_min.nc",lvar=1)
res=readRDS("index_list_wt.rds")
name_index=names(res)
month=sort(rep(c(1:12),7))
name_var_rr=paste0(gsub("ind","clim",names(res)),"_T1000")
name_var=paste0(gsub("ind_t_wt_*_","allclim_wt_",name_index),"_T1000_min", perl = TRUE)
name_var=gsub("TRUE","",name_var)
name_files_anomrr=paste0(gsub("ind","anomclim",names(res)),"_T1000_min.nc")
for (i in 1:length(res)) {
temp_index=as.numeric(unlist(res[name_index[i]]))
rr_temp_clim=subset(rr_clim,temp_index)
rr_temp_clim=calc(rr_temp_clim,mean,na.rm=T)
rr_anom_month=rr_temp_clim-rr_clim_m[[month[i]]]
writeRaster(rr_temp_clim,name_var_rr[i], "CDF", overwrite=TRUE,varname="T1000",varunit="K degree",longname="T1000",xname="lon",yname="lat")
writeRaster(rr_anom_month,name_files_anomrr[i], "CDF", overwrite=TRUE,varname="anom_T1000",varunit="degree",longname="T1000",xname="lon",yname="lat")
rr_temp_clim=subset(rr_clim,temp_index)
writeRaster(rr_temp_clim,name_var[i], "CDF", overwrite=TRUE,varname="T1000",varunit="K degree",longname="T1000",xname="lon",yname="lat")
}
###################################################################################################################################
library(raster)
library(rts)
setwd("/home/alf/Scrivania/lav_R_climate_seas/reanalisys")
################################################################
mgsub2 <- function(myrepl, mystring){
gsub2 <- function(l, x){
do.call('gsub', list(x = x, pattern = l[1], replacement = l[2]))
}
Reduce(gsub2, myrepl, init = mystring, right = T)
}
################################################################
name=c("T1000_avg",
"T1000_max",
"T1000_min",
"T850",
"hgt500")
T1000_avg_clim_wt=stack(sapply(Sys.glob("clim*T1000_avg*"),raster))
T1000_avg_clim_wt
Sys.glob("clim*T1000_avg*")
dir()
T1000_avg_clim_wt=stack(sapply(Sys.glob("clim_t_wt_*T1000_avg*"),raster))
Sys.glob("clim_t_wt_*T1000_avg*")
library(raster)
library(rts)
setwd("/home/alf/Scrivania/lav_R_climate_seas/reanalisys")
rr_clim=brick( "r_T1000_avgday_clim.nc",lvar=1)
rr_clim_m=brick( "climean_rea2_T1000_avg.nc",lvar=1)
res=readRDS("index_list_wt.rds")
name_index=names(res)
month=sort(rep(c(1:12),7))
name_var_rr=paste0(gsub("ind","clim",names(res)),"_T1000_avg")
name_var=paste0(gsub("ind_t_wt_*_","allclim_wt_",name_index),"_T1000_avg", perl = TRUE)
name_var=gsub("TRUE","",name_var)
name_files_anomrr=paste0(gsub("ind","anomclim",name_index),"_T1000_avg.nc")
for (i in 1:length(res)) {
temp_index=as.numeric(unlist(res[name_index[i]]))
rr_temp_clim=subset(rr_clim,temp_index)
rr_temp_clim=calc(rr_temp_clim,mean,na.rm=T)
rr_anom_month=rr_temp_clim-rr_clim_m[[month[i]]]
writeRaster(rr_temp_clim,name_var_rr[i], "CDF", overwrite=TRUE,varname="T1000",varunit="K degree",longname="T1000",xname="lon",yname="lat")
#   writeRaster(rr_anom_month,name_files_anomrr[i], "CDF", overwrite=TRUE,varname="anom_T1000",varunit="degree",longname="T1000",xname="lon",yname="lat")
#
#   rr_temp_clim=subset(rr_clim,temp_index)
#
#   writeRaster(rr_temp_clim,name_var[i], "CDF", overwrite=TRUE,varname="T1000",varunit="K degree",longname="T1000",xname="lon",yname="lat")
#
}
###################################################################################################################################
rr_clim=brick( "r_T1000_maxday_clim.nc",lvar=1)
rr_clim_m=brick( "climean_rea2_T1000_max.nc",lvar=1)
res=readRDS("index_list_wt.rds")
name_index=names(res)
month=sort(rep(c(1:12),7))
name_var_rr=paste0(gsub("ind","clim",names(res)),"_T1000_max")
name_var=paste0(gsub("ind_t_wt_*_","allclim_wt_",name_index),"_T1000_max", perl = TRUE)
name_var=gsub("TRUE","",name_var)
name_files_anomrr=paste0(gsub("ind","anomclim",name_index),"_T1000_max.nc")
for (i in 1:length(res)) {
temp_index=as.numeric(unlist(res[name_index[i]]))
rr_temp_clim=subset(rr_clim,temp_index)
rr_temp_clim=calc(rr_temp_clim,mean,na.rm=T)
rr_anom_month=rr_temp_clim-rr_clim_m[[month[i]]]
writeRaster(rr_temp_clim,name_var_rr[i], "CDF", overwrite=TRUE,varname="T1000",varunit="K degree",longname="T1000",xname="lon",yname="lat")
#   writeRaster(rr_anom_month,name_files_anomrr[i], "CDF", overwrite=TRUE,varname="anom_T1000",varunit="degree",longname="T1000",xname="lon",yname="lat")
#
#   rr_temp_clim=subset(rr_clim,temp_index)
#
#   writeRaster(rr_temp_clim,name_var[i], "CDF", overwrite=TRUE,varname="T1000",varunit="K degree",longname="T1000",xname="lon",yname="lat")
#
}
###################################################################################################################################
rr_clim=brick( "r_T1000_minday_clim.nc",lvar=1)
rr_clim_m=brick( "climean_rea2_T1000_min.nc",lvar=1)
res=readRDS("index_list_wt.rds")
name_index=names(res)
month=sort(rep(c(1:12),7))
name_var_rr=paste0(gsub("ind","clim",names(res)),"_T1000_min")
name_var=paste0(gsub("ind_t_wt_*_","allclim_wt_",name_index),"_T1000_min", perl = TRUE)
name_var=gsub("TRUE","",name_var)
name_files_anomrr=paste0(gsub("ind","anomclim",names(res)),"_T1000_min.nc")
for (i in 1:length(res)) {
temp_index=as.numeric(unlist(res[name_index[i]]))
rr_temp_clim=subset(rr_clim,temp_index)
rr_temp_clim=calc(rr_temp_clim,mean,na.rm=T)
rr_anom_month=rr_temp_clim-rr_clim_m[[month[i]]]
writeRaster(rr_temp_clim,name_var_rr[i], "CDF", overwrite=TRUE,varname="T1000",varunit="K degree",longname="T1000",xname="lon",yname="lat")
#   writeRaster(rr_anom_month,name_files_anomrr[i], "CDF", overwrite=TRUE,varname="anom_T1000",varunit="degree",longname="T1000",xname="lon",yname="lat")
#
#   rr_temp_clim=subset(rr_clim,temp_index)
#   writeRaster(rr_temp_clim,name_var[i], "CDF", overwrite=TRUE,varname="T1000",varunit="K degree",longname="T1000",xname="lon",yname="lat")
}
###################################################################################################################################
library(MASS)
library(robust)
library(xts)
library(car)
library(gvlma)
library(corrgram)
expo2015=read.csv("Mat_expo_2015.csv")
rownames(expo2015)=as.character(seq.Date(from=as.Date("2015-08-01"),to=as.Date("2015-09-27"),by=1))
library(lubridate)
setwd("/home/alf")
expo2015=read.csv("Mat_expo_2015.csv")
rownames(expo2015)=as.character(seq.Date(from=as.Date("2015-08-01"),to=as.Date("2015-09-27"),by=1))
library(forecast)
library(lubridate)
expo2015$wday=as.factor(wday(seq.Date(from=as.Date("2015-08-01"),to=as.Date("2015-09-27"),by=1)))
expo2015$wday
rownames(expo2015)=as.character(seq.Date(from=as.Date("2015-08-01"),to=as.Date("2015-09-27"),by=1))
expo2015_ts=as.xts(expo2015)
model_full=glm(I_expo_2015 ~ .,data=expo2015)
summary(model_full)
model_sel=stepAIC(model_full)
summary(lm(formula(model_sel),data=expo2015))
model_sel_up = update(model_sel, . ~ .  +1)
summary(lm(formula(model_sel_up),data=expo2015))
library(weatherData)
library("devtools")
install_github("Ram-N/weatherData")
library(weatherData)
?getWeatherForYear
library(MASS)
library(robust)
library(xts)
library(car)
library(gvlma)
library(corrgram)
library(forecast)
library(lubridate)
library(weatherData)
getStationCode("Italy")
milanodat <- getWeatherForDate("LIML", "2015-08-01", "2015-09-27")
milanodat
expo2015=merge(expo2015,milanodat[,2:4],all.x=T)
expo2015_ts=as.xts(expo2015)
model_full=glm(I_expo_2015 ~ .,data=expo2015)
summary(model_full)
str(expo2015)
model_sel=stepAIC(model_full)
summary(lm(formula(model_sel),data=expo2015))
vif(model_sel)
formula(model_sel)
str(expo2015)
summary(glm(formula(model_sel_up),family = quasipoisson(link = "log")),data=expo2015))
summary(glm(formula(model_sel_up),family = quasipoisson(link = "log")),data=expo2015)
summary(glm(formula(model_sel_up),family = quasipoisson(link = "log"),data=expo2015))
plot(glm(formula(model_sel_up),family = quasipoisson(link = "log"),data=expo2015))
plot(glm(formula(model_sel_up),family = quasipoisson(link = "log"),data=expo2015))
library(mgcv)
summary(gam(formula(model_sel),data=expo2015))
formula(model_sel)
summary(gam(I_expo_2015 ~ s(EXPO2015_RT) + EXPO2015_T + expo2015_RT + expo2016_All +
ExpoMilano2015_RT + ExpoMilano2016_All + Expo2015Milano_RT +
Expo2015Milano_All + Pad_Ita2015_RT + Pad_Ita2015_All + RaiExpo_All +
alberodellavita_RT + s(alberodellavita_All) + s(EXPO15_ratio) +
wday,data=expo2015))
summary(gam(I_expo_2015 ~ s(EXPO2015_RT) + s(EXPO2015_T) + expo2015_RT + expo2016_All +
ExpoMilano2015_RT + ExpoMilano2016_All + Expo2015Milano_RT +
Expo2015Milano_All + s(Pad_Ita2015_RT) + Pad_Ita2015_All + RaiExpo_All +
alberodellavita_RT + s(alberodellavita_All) + s(EXPO15_ratio) +
wday,data=expo2015))
summary(gam(I_expo_2015 ~ s(EXPO2015_RT) + s(EXPO2015_T)+ s(Pad_Ita2015_RT) + Pad_Ita2015_All + s(RaiExpo_All) +
alberodellavita_RT + s(alberodellavita_All) + s(EXPO15_ratio) +
wday,data=expo2015))
summary(gam(I_expo_2015 ~s(EXPO2015_T)+ + s(RaiExpo_All) +
+ s(alberodellavita_All) + s(EXPO15_ratio) +
wday,data=expo2015))
summary(gam(I_expo_2015 ~s(EXPO2015_T)+ + s(RaiExpo_All) +
alberodellavita_RT + s(alberodellavita_All) + s(EXPO15_ratio) +
wday,data=expo2015))
summary(gam(I_expo_2015 ~s(EXPO2015_T)+ + s(RaiExpo_All)  +s( expo2016_All)
alberodellavita_RT + s(alberodellavita_All) + s(EXPO15_ratio) +
wday,data=expo2015))
summary(gam(I_expo_2015 ~s(EXPO2015_T)+ + s(RaiExpo_All)  +s( expo2016_All)+
alberodellavita_RT + s(alberodellavita_All) + s(EXPO15_ratio) +
wday,data=expo2015))
summary(gam(I_expo_2015 ~s(EXPO2015_T)+ + s(RaiExpo_All)  +s( expo2016_All)+
s(alberodellavita_RT) + s(alberodellavita_All) + s(EXPO15_ratio) +
wday,data=expo2015))
model_spline_sel=gam(I_expo_2015 ~s(EXPO2015_T)+ + s(RaiExpo_All) +s(expo2016_All)+
s(alberodellavita_RT) + s(alberodellavita_All) + s(EXPO15_ratio) +
wday,data=expo2015)
plot(model_spline_sel)
plot(model_spline_sel)
s(alberodellavita_RT) + s(alberodellavita_All) + s(EXPO15_ratio) +
wday,data=expo2015))
summary(gam(I_expo_2015 ~s(EXPO2015_T) +s(expo2016_All)+
s(alberodellavita_RT) + s(alberodellavita_All) + s(EXPO15_ratio) +
wday,data=expo2015))
summary(lm(formula(model_sel),data=expo2015))
install.packages("benford.analisys")
devtools::install_github("cran/benford.analysis")
library(benford.analisys)
library("benford.analisys")
library("benford.analysis")
setwd("/home/alf")
expo2015=read.csv("Mat_expo_2015.csv")
expo2015[1]
bfd.cp <- benford(expo2015[1]) #generates benford object
bfd.cp #prints
bfd.cp <- benford(as.numeric(expo2015[,1]) #generates benford object
bfd.cp <- benford(as.numeric(expo2015[,1]))
bfd.cp
plot(bfd.cp) #plots
bfd.cp <- benford(as.numeric(expo2015[,1]),number.of.digits = 5) #generates benford object
bfd.cp #prints
plot(bfd.cp) #plots
devtools::install_github("cran/benford.analysis")
library("benford.analysis")
setwd("/home/alf")
expo2015=read.csv("Mat_expo_2015.csv")
data(corporate.payment) #loads data
bfd.cp <- benford(as.numeric(expo2015[,1]),number.of.digits = 2) #generates benford object
bfd.cp #prints
plot(bfd.cp) #plots
library(rAedesSim)
data(trappola_wmodel)
data(Punta_Ala_P7_meteo)
data(Follonica_P8_meteo)
data(Piombino_P7_meteo)
data(San_Vincenzo_P13_meteo)
data(Cecina_P17_meteo)
data(Vada_P20_meteo)
data(Cecina_P17_meteo)
data(Livorno_P20_meteo)
data(Pisa_P33_meteo)
data(Lido_Camaiore_P40_meteo)
data(Marinella_di_Sarzana_P46_meteo)
###
data(Livorno_P25_meteo)
data(Livorno_P25_meteo)
devtools::install_github("adamhsparks/GSODR
")
devtools::install_github("adamhsparks/GSODR")
library(GSODR)
devtools::install_github("environmentalinformatics-marburg/GSODTools")
shp_kibo <- GSODTools::stationFromCoords(x = 37.359031, y = -3.065053, width = 500)
library(GSODTools)
devtools::install_github("environmentalinformatics-marburg/GSODTools")
library(GSODTools)
shp_kibo <- stationFromCoords(x = 37.359031, y = -3.065053, width = 500)
library(timeDate)
install.packages("timedate")
install.packages("timeDate")
install.packages("timeDate")
library(GSODTools)
gsodstations
firenze <- filter(gsodstations, STATION.NAME == "FIRENZE")
firenze
firenze <- filter(gsodstations, STATION.NAME == "PERETOLA")
firenze
firenze <- filter(gsodstations, STATION.NAME == "PISA")
firenze
firenze <- filter(gsodstations, STATION.NAME == "SANGIUSTO")
firenze
head(gsodstations)
gsodstations$CTRY
which(gsodstations$CTRY =="IT")
which(gsodstations$CTRY =="ITA")
levels(gsodstations$CTRY)
which(gsodstations$CTRY =="IY")
which(gsodstations$CTRY =="IT")
levels(gsodstations$CTRY)
gsodstations
firenze <- filter(gsodstations, STATION.NAME == "FIRENZE")
firenze
library(weatherData)
palermo_code=getStationCode("Cochin")
palermo_code
palermo_code=getStationCode("Catania")
palermo_code
palermo_code=getStationCode("Palermo")
Sys.Date()
data_available_today=showAvailableColumns("LICJ", Sys.Date(), opt_detailed=T)
data_available_today
data_available_today_CT=showAvailableColumns("LICC", Sys.Date(), opt_detailed=T)
getDetailedWeather("LICJ", Sys.Date(), opt_all_columns=T)
Giarre_data_today=getDetailedWeather("IMASCALI2",Sys.Date(), opt_all_columns=T)
Giarre_data_today=getDetailedWeather("IMASCALI2",Sys.Date(), opt_all_columns=T,station_type="id")
Giarre_data_today
Barcellona_data_today=getDetailedWeather("IMESSINA4",Sys.Date(), opt_all_columns=T,station_type="id")
Barcellona_data_today
library(weatherData)
data(IntlWxStations)
IntlWxStations
ls_message_df=data.frame(data=channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)],
message=channel_obj$text[which(!duplicated(channel_obj$text)==TRUE)],
authors=channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)],
retweetCount=channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],
is.retweet=ls_retweet[which(!duplicated(channel_obj$text)==TRUE)])
rank_authors_retweet=aggregate(ls_message_df$retweetCount,list(ls_message_df$authors),sum)
rank_authors=rank_authors_retweet[order(-rank_authors_retweet[,2]),]
names(rank_authors_retweet)<-c("authors","retweetCount")
rank_message_retweet=aggregate(ls_message_df$retweetCount,list(ls_message_df$message),sum)
rank_message_retweet=rank_message_retweet[order(-rank_message_retweet[,2]),]
names(rank_message_retweet)<-c("message","retweetCount")
rank_authors_retweet=rank_authors_retweet[ order(-rank_authors_retweet[,2]), ]
rank_message_retweet=rank_message_retweet[ order(-rank_message_retweet[,2]), ]
##########################################################################################################################################
# retrieve other information from channel stack.
rank_message_retweet$retweeted_authors=retweeted_users(rank_message_retweet$message)
id_na_message_retweet=which(is.na(rank_message_retweet$retweeted_authors))
not_retweet_with_authors=as.character(rank_message_retweet$message[id_na_message_retweet])
replies_id=grep("^@",channel_obj$text)
channel_obj$replies=NA
channel_obj$replies[replies_id]=1
ls_replies_df=data.frame(data=channel_obj$data,authors=channel_obj$screeName,replies=channel_obj$replies)
####################################################################################
# Replies stats
fullretweet_day=aggregate(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],list(channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)]),sum,na.rm = TRUE)
names(fullretweet_day)=c("date","retweetCount")
fullretweet_day$date=as.Date(fullretweet_day$date)
fullreplies_day=aggregate(channel_obj$replies,list(channel_obj$data),sum,na.rm = TRUE)
names(fullreplies_day)=c("date","repliesCount")
fullreplies_day$date=as.Date(fullreplies_day$date)
fullretweet_missing=length(which(is.na(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)])))
fullretweet_channel_stat_sum=sum(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],na.rm=T)
replies_channel_stat_sum=length(replies_id)
#######################################################################################
# Create data.frame date,message and authors.
ls_favorite_df=data.frame(data=channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)],
message=channel_obj$text[which(!duplicated(channel_obj$text)==TRUE)],
authors=channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)],
favoriteCount=channel_obj$favoriteCount[which(!duplicated(channel_obj$text)==TRUE)],
is.retweet=ls_retweet[which(!duplicated(channel_obj$text)==TRUE)])
day_favorite=aggregate(ls_favorite_df$favoriteCount,list(ls_favorite_df$data),sum)
names(day_favorite)<-c("date","N_favor")
day_favorite$date=as.Date(day_favorite$date)
ls_favorite_df=ls_favorite_df[order(-ls_favorite_df$favoriteCount),]
rank_authors_favorite=aggregate(channel_obj$favoriteCount[which(!duplicated(channel_obj$text)==TRUE)],
list(channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)])
,sum)
rank_authors_favorite=rank_authors_favorite[order(-rank_authors_favorite[,2]),]
names(rank_authors_favorite)<-c("authors","favoriteCount")
#########################################################################
ls_message_df=data.frame(data=channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)],
message=channel_obj$text[which(!duplicated(channel_obj$text)==TRUE)],
authors=channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)],
retweetCount=channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],
is.retweet=ls_retweet[which(!duplicated(channel_obj$text)==TRUE)])
rank_authors_retweet=aggregate(ls_message_df$retweetCount,list(ls_message_df$authors),sum)
rank_authors=rank_authors_retweet[order(-rank_authors_retweet[,2]),]
names(rank_authors_retweet)<-c("authors","retweetCount")
rank_message_retweet=aggregate(ls_message_df$retweetCount,list(ls_message_df$message),sum)
rank_message_retweet=rank_message_retweet[order(-rank_message_retweet[,2]),]
names(rank_message_retweet)<-c("message","retweetCount")
rank_authors_retweet=rank_authors_retweet[ order(-rank_authors_retweet[,2]), ]
rank_message_retweet=rank_message_retweet[ order(-rank_message_retweet[,2]), ]
##########################################################################################################################################
# retrieve other information from channel stack.
rank_message_retweet$retweeted_authors=retweeted_users(rank_message_retweet$message)
id_na_message_retweet=which(is.na(rank_message_retweet$retweeted_authors))
not_retweet_with_authors=as.character(rank_message_retweet$message[id_na_message_retweet])
for ( i in seq_along(not_retweet_with_authors))
{
rank_message_retweet$retweeted_authors[id_na_message_retweet[i]]=as.character(channel_obj$screeName[min(which(channel_obj$text ==not_retweet_with_authors[i]))])
}
rank_message_retweet$data=NA
for ( i in seq_along(rank_message_retweet$message))
{
rank_message_retweet$data[i]=as.character(channel_obj$data[min(which((channel_obj$text %in%  rank_message_retweet$message[i] )==T))])
}
ls_hash=lapply(channel_obj$text,FUN=function(x) qdapRegex::rm_hash(x,extract=T))
ls_tag=lapply(channel_obj$text,FUN=function(x) extract_mentions(x))
ls_links=lapply(channel_obj$text,FUN=function(x) qdapRegex::rm_url(x, extract=TRUE))
ls_lenhash=unlist(lapply(ls_hash,FUN=function(x) ifelse(is.na(x),0, length(qdapRegex::rm_hash(x,extract=T)[[1]]))))
ls_lenlinks=unlist(lapply( ls_links,FUN=function(x) ifelse(is.na(x),0, length(qdapRegex::rm_url(x, extract=TRUE)[[1]]))))
ls_lentag=unlist(lapply(ls_tag,FUN=function(x) ifelse(is.na(x),0, length(extract_mentions(x)[[1]]))))
ls_words=unlist(lapply(channel_obj$text,FUN=function(x) qdap::word_count(x)))
####################################################################################
# Extract replies and organize a frame
replies_id=grep("^@",channel_obj$text)
channel_obj$replies=NA
channel_obj$replies[replies_id]=1
ls_replies_df=data.frame(data=channel_obj$data,authors=channel_obj$screeName,replies=channel_obj$replies)
####################################################################################
# Replies stats
fullretweet_day=aggregate(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],list(channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)]),sum,na.rm = TRUE)
names(fullretweet_day)=c("date","retweetCount")
fullretweet_day$date=as.Date(fullretweet_day$date)
fullreplies_day=aggregate(channel_obj$replies,list(channel_obj$data),sum,na.rm = TRUE)
names(fullreplies_day)=c("date","repliesCount")
fullreplies_day$date=as.Date(fullreplies_day$date)
fullretweet_missing=length(which(is.na(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)])))
fullretweet_channel_stat_sum=sum(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],na.rm=T)
replies_channel_stat_sum=length(replies_id)
#######################################################################################
# Create data.frame date,message and authors.
ls_favorite_df=data.frame(data=channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)],
message=channel_obj$text[which(!duplicated(channel_obj$text)==TRUE)],
authors=channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)],
favoriteCount=channel_obj$favoriteCount[which(!duplicated(channel_obj$text)==TRUE)],
is.retweet=ls_retweet[which(!duplicated(channel_obj$text)==TRUE)])
day_favorite=aggregate(ls_favorite_df$favoriteCount,list(ls_favorite_df$data),sum)
names(day_favorite)<-c("date","N_favor")
day_favorite$date=as.Date(day_favorite$date)
ls_favorite_df=ls_favorite_df[order(-ls_favorite_df$favoriteCount),]
rank_authors_favorite=aggregate(channel_obj$favoriteCount[which(!duplicated(channel_obj$text)==TRUE)],
list(channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)])
,sum)
rank_authors_favorite=rank_authors_favorite[order(-rank_authors_favorite[,2]),]
names(rank_authors_favorite)<-c("authors","favoriteCount")
#########################################################################
remove.packages("rTwChannel", lib="~/R/x86_64-pc-linux-gnu-library/3.2")
remove.packages("rTwChannel", lib="~/R/x86_64-pc-linux-gnu-library/3.2")
devtools::install_github("alfcrisci/rTwChannel")
library(rTwChannel)
setwd("/home/alf/Scrivania/lav_ubu_grasso/newanalisys/full")
analitic_full_LIG=readRDS("data/analitic_full_LIG.rds")
analitic_full_TOS=readRDS("data/analitic_full_TOS.rds")
analitic_full_PIE=readRDS("data/analitic_full_PIE.rds")
period_channels=read.csv("data/period_channels.csv")
an_full_allertameteoPIE=channel_analytic(analitic_full_PIE,use_channel_dates = FALSE,start_date=period_channels$start_date[3],end_date=period_channels$end_date[3],Ntop=11)
channel_obj=an_full_allertameteoPIE
channel_obj=analitic_full_PIE
ls_retweeted_authors=lapply(channel_obj$text,FUN=function(x) qdapRegex::rm_default(x, pattern="RT @([:alnum:]*[_]*[:alnum:]*):",extract=T));
ls_retweeted_authors=unlist(lapply(ls_retweeted_authors,function(x) gsub(":","",gsub("^RT @","",x[1]))))
write.csv(ls_retweeted_authors,"ls_retweeted_authors.csv",row.names=F)
####################################################################################
# Create lists to be used for count statistics.
ls_hash=lapply(channel_obj$text,FUN=function(x) qdapRegex::rm_hash(x,extract=T))
ls_tag=lapply(channel_obj$text,FUN=function(x) extract_mentions(x))
ls_retweeted_df=data.frame(data=channel_obj$data,
retweeted_authors=as.character(ls_retweeted_authors),
authors=channel_obj$screeName)
write.csv(ls_retweeted_df,"ls_retweeted_df.csv",row.names=F)
detach("package:rTwChannel", unload=TRUE)
library("rTwChannel", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.2")
an_full_allertameteoPIE=channel_analytic(analitic_full_PIE,use_channel_dates = FALSE,start_date=period_channels$start_date[3],end_date=period_channels$end_date[3],Ntop=11)
devtools::install_github("alfcrisci/rTwChannel")
library(rTwChannel)
an_full_allertameteoPIE=channel_analytic(analitic_full_PIE,use_channel_dates = FALSE,start_date=period_channels$start_date[3],end_date=period_channels$end_date[3],Ntop=11)
an_full_allertameteoPIE=channel_analytic(analitic_full_PIE,use_channel_dates = FALSE,start_date=period_channels$start_date[3],end_date=period_channels$end_date[3],Ntop=11)
ls_retweeted_authors=lapply(channel_obj$text,FUN=function(x) return(stringr::str_extract((x, pattern="RT @([:alnum:]*[_]*[:alnum:]*):",extract=T)));
ls_retweeted_authors=lapply(channel_obj$text,FUN=function(x) return(stringr::str_extract((x, pattern="RT @([:alnum:]*[_]*[:alnum:]*):",extract=T))));
ls_retweeted_authors=lapply(channel_obj$text,FUN=function(x) return(stringr::str_extract((x, pattern="RT @([:alnum:]*[_]*[:alnum:]*):"))));
ls_retweeted_authors=lapply(channel_obj$text,FUN=function(x) return(stringr::str_extract(x, pattern="RT @([:alnum:]*[_]*[:alnum:]*):")));
ls_retweeted_authors
ls_retweeted_authors=unlist(lapply(ls_retweeted_authors,function(x) gsub(":","",gsub("^RT @","",x[1]))))
ls_retweeted_authors
channel_obj$data
length(channel_obj$data)
length(channel_obj$authors)
length(channel_obj$screeName)
ls_retweeted_df=data.frame(data=channel_obj$data,
retweeted_authors=ls_retweeted_authors,
authors=channel_obj$screeName)
ls_retweeted_df
ls_retweeted_df=na.omit(data.frame(data=channel_obj$data,
retweeted_authors=ls_retweeted_authors,
authors=channel_obj$screeName))
ls_retweeted_df
detach("package:rTwChannel", unload=TRUE)
remove.packages("rTwChannel", lib="~/R/x86_64-pc-linux-gnu-library/3.2")
devtools::install_github("alfcrisci/rTwChannel")
devtools::install_github("alfcrisci/rTwChannel")
library(rTwChannel)
an_full_allertameteoPIE=channel_analytic(analitic_full_PIE,use_channel_dates = FALSE,start_date=period_channels$start_date[3],end_date=period_channels$end_date[3],Ntop=11)
devtools::install_github("alfcrisci/rTwChannel")
an_full_allertameteoPIE=channel_analytic(analitic_full_PIE,use_channel_dates = FALSE,start_date=period_channels$start_date[3],end_date=period_channels$end_date[3],Ntop=11)
