firenze <- filter(gsodstations, STATION.NAME == "FIRENZE")
firenze
library(weatherData)
palermo_code=getStationCode("Cochin")
palermo_code
palermo_code=getStationCode("Catania")
palermo_code
palermo_code=getStationCode("Palermo")
Sys.Date()
data_available_today=showAvailableColumns("LICJ", Sys.Date(), opt_detailed=T)
data_available_today
data_available_today_CT=showAvailableColumns("LICC", Sys.Date(), opt_detailed=T)
getDetailedWeather("LICJ", Sys.Date(), opt_all_columns=T)
Giarre_data_today=getDetailedWeather("IMASCALI2",Sys.Date(), opt_all_columns=T)
Giarre_data_today=getDetailedWeather("IMASCALI2",Sys.Date(), opt_all_columns=T,station_type="id")
Giarre_data_today
Barcellona_data_today=getDetailedWeather("IMESSINA4",Sys.Date(), opt_all_columns=T,station_type="id")
Barcellona_data_today
library(weatherData)
data(IntlWxStations)
IntlWxStations
ls_message_df=data.frame(data=channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)],
message=channel_obj$text[which(!duplicated(channel_obj$text)==TRUE)],
authors=channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)],
retweetCount=channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],
is.retweet=ls_retweet[which(!duplicated(channel_obj$text)==TRUE)])
rank_authors_retweet=aggregate(ls_message_df$retweetCount,list(ls_message_df$authors),sum)
rank_authors=rank_authors_retweet[order(-rank_authors_retweet[,2]),]
names(rank_authors_retweet)<-c("authors","retweetCount")
rank_message_retweet=aggregate(ls_message_df$retweetCount,list(ls_message_df$message),sum)
rank_message_retweet=rank_message_retweet[order(-rank_message_retweet[,2]),]
names(rank_message_retweet)<-c("message","retweetCount")
rank_authors_retweet=rank_authors_retweet[ order(-rank_authors_retweet[,2]), ]
rank_message_retweet=rank_message_retweet[ order(-rank_message_retweet[,2]), ]
##########################################################################################################################################
# retrieve other information from channel stack.
rank_message_retweet$retweeted_authors=retweeted_users(rank_message_retweet$message)
id_na_message_retweet=which(is.na(rank_message_retweet$retweeted_authors))
not_retweet_with_authors=as.character(rank_message_retweet$message[id_na_message_retweet])
replies_id=grep("^@",channel_obj$text)
channel_obj$replies=NA
channel_obj$replies[replies_id]=1
ls_replies_df=data.frame(data=channel_obj$data,authors=channel_obj$screeName,replies=channel_obj$replies)
####################################################################################
# Replies stats
fullretweet_day=aggregate(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],list(channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)]),sum,na.rm = TRUE)
names(fullretweet_day)=c("date","retweetCount")
fullretweet_day$date=as.Date(fullretweet_day$date)
fullreplies_day=aggregate(channel_obj$replies,list(channel_obj$data),sum,na.rm = TRUE)
names(fullreplies_day)=c("date","repliesCount")
fullreplies_day$date=as.Date(fullreplies_day$date)
fullretweet_missing=length(which(is.na(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)])))
fullretweet_channel_stat_sum=sum(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],na.rm=T)
replies_channel_stat_sum=length(replies_id)
#######################################################################################
# Create data.frame date,message and authors.
ls_favorite_df=data.frame(data=channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)],
message=channel_obj$text[which(!duplicated(channel_obj$text)==TRUE)],
authors=channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)],
favoriteCount=channel_obj$favoriteCount[which(!duplicated(channel_obj$text)==TRUE)],
is.retweet=ls_retweet[which(!duplicated(channel_obj$text)==TRUE)])
day_favorite=aggregate(ls_favorite_df$favoriteCount,list(ls_favorite_df$data),sum)
names(day_favorite)<-c("date","N_favor")
day_favorite$date=as.Date(day_favorite$date)
ls_favorite_df=ls_favorite_df[order(-ls_favorite_df$favoriteCount),]
rank_authors_favorite=aggregate(channel_obj$favoriteCount[which(!duplicated(channel_obj$text)==TRUE)],
list(channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)])
,sum)
rank_authors_favorite=rank_authors_favorite[order(-rank_authors_favorite[,2]),]
names(rank_authors_favorite)<-c("authors","favoriteCount")
#########################################################################
ls_message_df=data.frame(data=channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)],
message=channel_obj$text[which(!duplicated(channel_obj$text)==TRUE)],
authors=channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)],
retweetCount=channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],
is.retweet=ls_retweet[which(!duplicated(channel_obj$text)==TRUE)])
rank_authors_retweet=aggregate(ls_message_df$retweetCount,list(ls_message_df$authors),sum)
rank_authors=rank_authors_retweet[order(-rank_authors_retweet[,2]),]
names(rank_authors_retweet)<-c("authors","retweetCount")
rank_message_retweet=aggregate(ls_message_df$retweetCount,list(ls_message_df$message),sum)
rank_message_retweet=rank_message_retweet[order(-rank_message_retweet[,2]),]
names(rank_message_retweet)<-c("message","retweetCount")
rank_authors_retweet=rank_authors_retweet[ order(-rank_authors_retweet[,2]), ]
rank_message_retweet=rank_message_retweet[ order(-rank_message_retweet[,2]), ]
##########################################################################################################################################
# retrieve other information from channel stack.
rank_message_retweet$retweeted_authors=retweeted_users(rank_message_retweet$message)
id_na_message_retweet=which(is.na(rank_message_retweet$retweeted_authors))
not_retweet_with_authors=as.character(rank_message_retweet$message[id_na_message_retweet])
for ( i in seq_along(not_retweet_with_authors))
{
rank_message_retweet$retweeted_authors[id_na_message_retweet[i]]=as.character(channel_obj$screeName[min(which(channel_obj$text ==not_retweet_with_authors[i]))])
}
rank_message_retweet$data=NA
for ( i in seq_along(rank_message_retweet$message))
{
rank_message_retweet$data[i]=as.character(channel_obj$data[min(which((channel_obj$text %in%  rank_message_retweet$message[i] )==T))])
}
ls_hash=lapply(channel_obj$text,FUN=function(x) qdapRegex::rm_hash(x,extract=T))
ls_tag=lapply(channel_obj$text,FUN=function(x) extract_mentions(x))
ls_links=lapply(channel_obj$text,FUN=function(x) qdapRegex::rm_url(x, extract=TRUE))
ls_lenhash=unlist(lapply(ls_hash,FUN=function(x) ifelse(is.na(x),0, length(qdapRegex::rm_hash(x,extract=T)[[1]]))))
ls_lenlinks=unlist(lapply( ls_links,FUN=function(x) ifelse(is.na(x),0, length(qdapRegex::rm_url(x, extract=TRUE)[[1]]))))
ls_lentag=unlist(lapply(ls_tag,FUN=function(x) ifelse(is.na(x),0, length(extract_mentions(x)[[1]]))))
ls_words=unlist(lapply(channel_obj$text,FUN=function(x) qdap::word_count(x)))
####################################################################################
# Extract replies and organize a frame
replies_id=grep("^@",channel_obj$text)
channel_obj$replies=NA
channel_obj$replies[replies_id]=1
ls_replies_df=data.frame(data=channel_obj$data,authors=channel_obj$screeName,replies=channel_obj$replies)
####################################################################################
# Replies stats
fullretweet_day=aggregate(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],list(channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)]),sum,na.rm = TRUE)
names(fullretweet_day)=c("date","retweetCount")
fullretweet_day$date=as.Date(fullretweet_day$date)
fullreplies_day=aggregate(channel_obj$replies,list(channel_obj$data),sum,na.rm = TRUE)
names(fullreplies_day)=c("date","repliesCount")
fullreplies_day$date=as.Date(fullreplies_day$date)
fullretweet_missing=length(which(is.na(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)])))
fullretweet_channel_stat_sum=sum(channel_obj$retweetCount[which(!duplicated(channel_obj$text)==TRUE)],na.rm=T)
replies_channel_stat_sum=length(replies_id)
#######################################################################################
# Create data.frame date,message and authors.
ls_favorite_df=data.frame(data=channel_obj$data[which(!duplicated(channel_obj$text)==TRUE)],
message=channel_obj$text[which(!duplicated(channel_obj$text)==TRUE)],
authors=channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)],
favoriteCount=channel_obj$favoriteCount[which(!duplicated(channel_obj$text)==TRUE)],
is.retweet=ls_retweet[which(!duplicated(channel_obj$text)==TRUE)])
day_favorite=aggregate(ls_favorite_df$favoriteCount,list(ls_favorite_df$data),sum)
names(day_favorite)<-c("date","N_favor")
day_favorite$date=as.Date(day_favorite$date)
ls_favorite_df=ls_favorite_df[order(-ls_favorite_df$favoriteCount),]
rank_authors_favorite=aggregate(channel_obj$favoriteCount[which(!duplicated(channel_obj$text)==TRUE)],
list(channel_obj$screeName[which(!duplicated(channel_obj$text)==TRUE)])
,sum)
rank_authors_favorite=rank_authors_favorite[order(-rank_authors_favorite[,2]),]
names(rank_authors_favorite)<-c("authors","favoriteCount")
#########################################################################
remove.packages("rTwChannel", lib="~/R/x86_64-pc-linux-gnu-library/3.2")
remove.packages("rTwChannel", lib="~/R/x86_64-pc-linux-gnu-library/3.2")
install.packages("Rselenium")
install.packages("RSelenium")
library(RSelenium)
download_channel=function(channel,outfile,format,start_date,end_date,user,pass) {
url_web=paste0("http://disit.org/tv/query/query.php?channel=",channel,"&start_date=",start_date,"&end_date=",end_date,"&format=",format)
extra_web=paste0("--user ",user," --password ",pass)
download.file(url_web,destfile=outfile,method="wget",extra = extra_web)
}
mese=c("03","04","05","06","07","08","09","10","11")
i=5
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
i=6
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
i=7
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
i=8
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="Allertameteo%20TOSCANA",
outfile=paste0("Allertameteo_20TOSCANA_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="CALDO",
outfile="CALDO_A.csv",
format="csv",
start_date="2015-03-01",
end_date="2015-05-01",
user="crisci",
pass="v5TTdsug")
mese=c("03","04","05","06","07","08","09","10","11")
for ( i in 3:8){
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],".csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
}
i=5
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
i=6
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
i=7
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
i=8
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_A.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-02"),
end_date=paste0("2015-",mese[i+1],"-15"),
user="crisci",
pass="v5TTdsug")
download_channel(channel="CALDO",
outfile=paste0("CALDO_",mese[i],"_B.csv"),
format="csv",
start_date=paste0("2015-",mese[i],"-16"),
end_date=paste0("2015-",mese[i+1],"-01"),
user="crisci",
pass="v5TTdsug")
install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
library(tm.plugin.sentiment)
install.packages("tm.plugin.sentiment")
install.packages("tm.plugin.sentiment", repos="http://R-Forge.R-project.org")
library(tm.lexicon.GeneralInquirer)
# install.packages("tm.plugin.sentiment", repos="http://R-Forge.R-project.org")
library(tm.plugin.sentiment) # posted comments on SO about this not working
library(tm)
Here is a starting point. First is some code to install the sentiment plug ins (thank you, Dason, for the useful comment).
Next, with some text from a previous SO post to show what you might do, you can create a data frame.
Install the packages for sentiment analysis:
# install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
library(tm.lexicon.GeneralInquirer)
# install.packages("tm.plugin.sentiment", repos="http://R-Forge.R-project.org")
library(tm.plugin.sentiment) # posted comments on SO about this not working
library(tm)
Using the installed functions:
some_txt<- c("I am very happy at stack overflow , excited, and optimistic.",
"I am very scared from OP question, annoyed, and irritated.", "I am completely neutral about blandness.")
corpus <- Corpus(VectorSource(some_txt))
pos <- sum(sapply(corpus, tm_term_score, terms_in_General_Inquirer_categories("Positiv")))
neg <- sum(sapply(corpus, tm_term_score, terms_in_General_Inquirer_categories("Negativ")))
pos.score <- tm_term_score(TermDocumentMatrix(corpus, control = list(removePunctuation = TRUE)),
terms_in_General_Inquirer_categories("Positiv")) # this lists each document with number below
neg.score <- tm_term_score(TermDocumentMatrix(corpus, control = list(removePunctuation = TRUE)),
terms_in_General_Inquirer_categories("Negativ"))
pos.score
?terms_in_General_Inquirer_categories
library(rpart)
citation("rpart")
citation("rpart.plot")
citation("Deducer")
citation("ROCR")
library(twitteR)
#sql_lite_file=tempfile()
#register_sqlite_backend(sql_lite_file)
consumer_key=c('7lTvtNwLaDKTC0E0owLQlQ')
consumer_secret=c('PWnxtLd7QcerU5BkvflrJvdZb6hmFkIPyvygevNvkg')
access_token=c('1722659330-TjOcHVgpJtnm07IJq0SQlYex9EH8SrtHCBiPQNn')
access_secret=c('aacyGGwNs33fcRtztVdjDgXcuCJWIVSTgB7VZzkM4wFMO')
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
# Use a local file to cache OAuth access credentials between R sessions?
# 1: Yes
# 2: No
tweets<-searchTwitter('#pasocial',n=1500)
saveRDS(tweets,paste0("iononrischio",Sys.Date(),".rds"))
library(twitteR)
#sql_lite_file=tempfile()
#register_sqlite_backend(sql_lite_file)
consumer_key=c('7lTvtNwLaDKTC0E0owLQlQ')
consumer_secret=c('PWnxtLd7QcerU5BkvflrJvdZb6hmFkIPyvygevNvkg')
access_token=c('1722659330-TjOcHVgpJtnm07IJq0SQlYex9EH8SrtHCBiPQNn')
access_secret=c('aacyGGwNs33fcRtztVdjDgXcuCJWIVSTgB7VZzkM4wFMO')
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
# Use a local file to cache OAuth access credentials between R sessions?
# 1: Yes
# 2: No
tweets<-searchTwitter('#pasocial',n=1500)
tweets
tweets[1500]
str(tweets[1500])
tweets<-searchTwitter('#pasocial',maxID="666536123162849280",n=1500)
tweets
tweets<-searchTwitter('#pasocial',n=1500)
tweets_a<-searchTwitter('#pasocial',maxID="666536123162849280",n=1500)
tweets_df=twListToDF(tweets)
tweets_a_df=twListToDF(tweets_a)
all_tweets=rbind(tweets_df,tweets_a_df)
all_tweets
tweets
tweets_df=twListToDF(tweets)
tweets_df
nrow(all_tweets)
write.csv(all_tweets,"pasocial_1200_17_11.csv")
library(rTwChannel)
?channel_analytic()
?channel_analytic
channel_analytic
an_pasocial=channel_analytic(all_tweets, use_channel_dates = TRUE,
Ntop = 11, temporal_check = FALSE, Nmin = 25, naming ="twitter", stopword = tm::stopwords("it"),corpus_hashtag = TRUE)
iconv(all_tweets$text,"utf-8")
all_tweets$text=iconv(all_tweets$text,"utf-8")
an_pasocial=channel_analytic(all_tweets, use_channel_dates = TRUE,
Ntop = 11, temporal_check = FALSE, Nmin = 25, naming ="twitter", stopword = tm::stopwords("it"),corpus_hashtag = TRUE)
an_pasocial=channel_analytic(all_tweets, use_channel_dates = TRUE,only_original_tweet = TRUE,
Ntop = 11, temporal_check = FALSE, Nmin = 25, naming ="twitter", stopword = tm::stopwords("it"),corpus_hashtag = TRUE)
names(all_tweets)
all_tweets$isRetweet
as.integer(all_tweets$isRetweet)
all_tweets$isRetweet=as.integer(all_tweets$isRetweet)
an_pasocial=channel_analytic(all_tweets, use_channel_dates = TRUE,
Ntop = 11, temporal_check = FALSE, Nmin = 25, naming ="twitter", stopword = tm::stopwords("it"),corpus_hashtag = TRUE)
channel_obj=all_tweets
use_channel_dates = TRUE
channel_obj$data=as.Date(channel_obj$created)
channel_obj$hour=lubridate::hour(channel_obj$created)
channel_obj$month=lubridate::month(channel_obj$created)
channel_obj$data
ls_retweet=unlist(lapply(channel_obj$text,FUN=function(x) is.retweet(x)))
ls_links=lapply(channel_obj$text,FUN=function(x) qdapRegex::rm_url(x, extract=TRUE))
ls_retweeted_authors=sapply(channel_obj$text,FUN=function(x) retweeted_users(x));
if ( lowercase == TRUE) {
channel_obj$text=tolower(channel_obj$text)
}
ls_hash=lapply(channel_obj$text,FUN=function(x) qdapRegex::rm_hash(x,extract=T))
ls_tag=lapply(channel_obj$text,FUN=function(x) extract_mentions(x))
ls_lenhash=unlist(lapply(ls_hash,FUN=function(x) ifelse(is.na(x),0, length(qdapRegex::rm_hash(x,extract=T)[[1]]))))
ls_lenlinks=unlist(lapply(ls_links,FUN=function(x) ifelse(is.na(x),0, length(qdapRegex::rm_url(x, extract=TRUE)[[1]]))))
ls_lentag=unlist(lapply(ls_tag,FUN=function(x) ifelse(is.na(x),0, length(extract_mentions(x)[[1]]))))
ls_words=unlist(lapply(channel_obj$text,FUN=function(x) qdap::word_count(x)))
channel_obj$text=tolower(channel_obj$text)
ls_retweeted_df=na.omit(data.frame(data=channel_obj$data,
retweeted_authors=ls_retweeted_authors,
authors=channel_obj$screeName))
ls_retweeted_authors
ls_retweeted_authors
ls_retweeted_authors=sapply(channel_obj$text,FUN=function(x) retweeted_users(x));
ls_retweeted_authors
all_tweets=rbind(tweets_df,tweets_a_df)
all_tweets$text=iconv(all_tweets$text,"utf-8")
all_tweets$isRetweet=as.integer(all_tweets$isRetweet)
all_tweets$text
retweeted_users(all_tweets[2171])
retweeted_users(all_tweets$text[2171])
ls_retweeted_authors=sapply(channel_obj$text,FUN=function(x) retweeted_users(x));
ls_retweeted_authors
class(ls_retweeted_authors)
ls_retweeted_authors=sapply(channel_obj$text,FUN=function(x) retweeted_users(x));
ls_retweeted_authors
ls_retweeted_authors[2]
ls_retweeted_authors=sapply(channel_obj$text,FUN=retweeted_users);
ls_retweeted_authors
retweeted_users(channel_obj$text)
ls_retweeted_authors=lapply(as.list(channel_obj$text),FUN=retweeted_users);
ls_retweeted_authors
ls_retweeted_authors=sapply(as.character(channel_obj$text),FUN=retweeted_users);
ls_retweeted_authors
retweeted_users(channel_obj$text[4])
channel_obj$text[2171]
all_tweets=rbind(tweets_df,tweets_a_df)
all_tweets$text=iconv(all_tweets$text,"utf-8")
all_tweets$isRetweet=as.integer(all_tweets$isRetweet)
all_tweets$text
ls_retweet=unlist(lapply(channel_obj$text,FUN=function(x) is.retweet(x)))
ls_retweeted_authors=sapply(channel_obj$text,FUN=function(x) retweeted_users(x));
ls_retweeted_authors
all_tweets$text
retweeted_users(all_tweets$text[2185])
for ( i in 1:length(all_tweets$text) {ls_retweeted_authors[i]=retweeted_users(all_tweets$text[i])}
for ( i in 1:length(all_tweets$text)) {ls_retweeted_authors[i]=retweeted_users(all_tweets$text[i]) }
ls_retweeted_authors
ls_retweeted_authors=as.character(rep(NA,nrow(all_tweets)))
ls_retweeted_authors
detach("package:rTwChannel", unload=TRUE)
remove.packages("rTwChannel", lib="~/R/x86_64-pc-linux-gnu-library/3.2")
devtools::install_github("alfcrisci/rTwChannel")
library(rTwChannel)
tweets<-searchTwitter('#pasocial',n=1500)
length(tweets)
tweets[[length(tweets)]]$id
tweets_a<-searchTwitter('#pasocial',maxID=tweets[[length(tweets)]]$id,n=1500)
tweets_a
tweets_df=twListToDF(tweets)
tweets_a_df=twListToDF(tweets_a)
all_tweets=rbind(tweets_df,tweets_a_df)
an_pasocial=channel_analytic(all_tweets, use_channel_dates = TRUE,Ntop = 11, temporal_check = FALSE, Nmin = 25, naming ="twitter", stopword = tm::stopwords("it"),corpus_hashtag = TRUE)
Ntop = 11, temporal_check = FALSE, Nmin = 25, naming ="twitter", stopword = tm::stopwords("it"),corpus_hashtag = TRUE)
channel_obj=all_tweets
channel_obj
channel_obj$data=as.Date(channel_obj$created)
channel_obj$hour=lubridate::hour(channel_obj$created)
channel_obj$month=lubridate::month(channel_obj$created)
channel_obj$text=iconv(all_tweets$text,"utf-8")
channel_obj$isRetweet=as.integer(all_tweets$isRetweet)
ls_retweet=unlist(lapply(channel_obj$text,FUN=function(x) is.retweet(x)))
ls_retweeted_authors=as.character(rep(NA,nrow(channel_obj)))
ls_retweet
ls_retweet=unlist(lapply(channel_obj$text,FUN=function(x) is.retweet(x)))
ls_retweeted_authors
for ( i in 1:length(ls_retweeted_authors)) {ls_retweeted_authors[i]=retweeted_users(all_tweets$text[i]) }
ls_retweeted_authors
ls_retweeted_df=na.omit(data.frame(data=channel_obj$data,
retweeted_authors=ls_retweeted_authors,
authors=channel_obj$screeName))
channel_obj$screeName
names(all_tweets)
devtools::install_github("alfcrisci/rTwChannel")
library(rTwChannel)
an_pasocial=channel_analytic(all_tweets, use_channel_dates = TRUE,Ntop = 11, temporal_check = FALSE, Nmin = 25, naming ="twitter", stopword = tm::stopwords("it"),corpus_hashtag = TRUE)
saveRDS(an_pasocial,"an_pasocial.rds")
names(an_pasocial)
an_pasocial$table_hash
channel_outputs(an_pasocial,param="channel_stat",suffix_file="#pasocial")
channel_outputs(an_pasocial,param="top_authors",suffix_file="#pasocial")
channel_outputs(an_pasocial,param="top_message",suffix_file="#pasocial")
channel_outputs(an_pasocial,param="table_authors",suffix_file="#pasocial")
channel_outputs(an_pasocial,param="table_hash",suffix_file="#pasocial")
channel_outputs(an_pasocial,param="table_mentions",suffix_file="#pasocial")
channel_outputs(an_pasocial,param="table_message",suffix_file="#pasocial")
channel_outputs(an_pasocial,param="table_authors",suffix_file="#pasocial")
channel_outputs(an_pasocial,param="table_authors_retweeter",suffix_file="#pasocial")
channel_outputs(an_pasocial,param="table_authors_retweeted",suffix_file="#pasocial")
channel_outputs(an_pasocial,param="table_authors_retweeter",suffix_file="#pa")
channel_outputs(an_pasocial,param="table_authors_retweeted",suffix_file="#pa")
devtools::install_github("ironholds/pageviews", ref = "0.1.0")
library(rTwChannel)
setwd("/home/alf/Scrivania/lav_ubu_grasso/newanalisys/full")
analitic_full_LIG=readRDS("data/analitic_full_LIG.rds")
analitic_full_TOS=readRDS("data/analitic_full_TOS.rds")
analitic_full_PIE=readRDS("data/analitic_full_PIE.rds")
period_channels=read.csv("data/period_channels.csv")
analitic_full_LIG$isRetweet=unlist(lapply(analitic_full_LIG$text,FUN=function(x) is.retweet(x)))
analitic_full_TOS$isRetweet=unlist(lapply(analitic_full_TOS$text,FUN=function(x) is.retweet(x)))
analitic_full_PIE$isRetweet=unlist(lapply(analitic_full_PIE$text,FUN=function(x) is.retweet(x)))
analitic_full_LIG$screenName=analitic_full_LIG$screeName
analitic_full_TOS$screenName=analitic_full_TOS$screeName
analitic_full_PIE$screenName=analitic_full_PIE$screeName
setwd("/home/alf/Scrivania/lav_ubu_grasso/newanalisys/native")
dir()
an_native_allertameteoTOS=channel_analytic(native_channel(analitic_full_TOS),use_channel_dates = FALSE,start_date=period_channels$start_date[2],end_date=period_channels$end_date[2],Ntop=11,only_original_tweet=TRUE,naming="twitter")
an_native_allertameteoPIE=channel_analytic(native_channel(analitic_full_PIE),use_channel_dates = FALSE,start_date=period_channels$start_date[3],end_date=period_channels$end_date[3],Ntop=11,only_original_tweet=TRUE,naming="twitter")
saveRDS(an_native_allertameteoTOS,"data/an_native_allertameteoTOS.rds")
saveRDS(an_native_allertameteoPIE,"data/an_native_allertameteoPIE.rds")
an_native_allertameteoLIG=channel_analytic(native_channel(analitic_full_LIG),use_channel_dates = FALSE,start_date=period_channels$start_date[1],end_date=period_channels$end_date[1],Ntop=11,only_original_tweet=TRUE,naming="twitter")
saveRDS(an_native_allertameteoLIG,"data/an_native_allertameteoLIG.rds")
